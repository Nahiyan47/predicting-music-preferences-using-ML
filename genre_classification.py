# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EfeJYCUnObgpYNk4q_IWrqlP_GdSGTj4
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

# Custom function to convert string representations of views to numeric values
def convert_views(views_str):
    if views_str[-1] == 'K':
        return float(views_str[:-1]) * 1000
    elif views_str[-1] == 'M':
        return float(views_str[:-1]) * 1000000
    elif views_str[-1] == 'B':
        return float(views_str[:-1]) * 1000000000
    else:
        return float(views_str)

# Load the dataset
data = pd.read_csv('updated_song_details.csv')

# Check for missing values in relevant features
relevant_features = ['song_name', 'author', 'release_year', 'lyrics', 'keywords', 'emotional_theme', 'tempo', 'instruments', 'youtube_views', 'genre', 'polarity', 'subjectivity', 'compound', 'positive', 'negative']
data_relevant = data[relevant_features].copy()  # Create a copy of the relevant subset

# Drop rows with missing values in relevant features
data_relevant.dropna(inplace=True)

# Convert youtube_views to numeric values using .loc for assignment
data_relevant.loc[:, 'youtube_views'] = data_relevant['youtube_views'].apply(convert_views)

# Define features and target variable
X = data_relevant.drop(columns=['song_name', 'genre'])
y = data_relevant['genre']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define preprocessing pipeline
numeric_features = ['release_year', 'tempo', 'polarity', 'subjectivity', 'compound', 'positive', 'negative']
categorical_features = ['emotional_theme', 'instruments']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Define the model pipeline
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Train the model
model.fit(X_train, y_train)

# Predict on test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Overall Accuracy: {accuracy:.2f}")

# Extract feature names after one-hot encoding
feature_names_after_encoding = list(model.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(input_features=categorical_features))

# Combine numeric features and one-hot encoded categorical feature names
all_feature_names = numeric_features + feature_names_after_encoding

# Get feature importances from the classifier
importances = model.named_steps['classifier'].feature_importances_

# Create a DataFrame to store feature importances with their corresponding feature names
importance_df = pd.DataFrame({'Feature': all_feature_names, 'Importance': importances})

# Sort feature importances in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the top three most important features
top_three_features = importance_df.head(3)

plt.figure(figsize=(10, 6))
plt.barh(top_three_features['Feature'], top_three_features['Importance'], color='skyblue')
plt.xlabel('Importance')
plt.title('Top Three Most Important Features')
plt.gca().invert_yaxis()  # Invert y-axis to display highest importance at the top
plt.show()





